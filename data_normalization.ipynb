{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization with Keras ImageDataGenerators\n",
    "# https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# example of loading the MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train (60000, 28, 28) (60000,)\n",
      "Test ((10000, 28, 28), (10000,))\n",
      "Train 0 255 33.318421449829934 78.56748998339798\n",
      "Test 0 255 33.791224489795916 79.17246322228644\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's have a look to the dataset\n",
    "#\n",
    "# load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# summarize dataset shape\n",
    "print('Train', train_images.shape, train_labels.shape)\n",
    "print('Test', (test_images.shape, test_labels.shape))\n",
    "# summarize pixel values\n",
    "print('Train', train_images.min(), train_images.max(), train_images.mean(), train_images.std())\n",
    "print('Test', test_images.min(), test_images.max(), test_images.mean(), test_images.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train (60000, 28, 28)\nTest (60000, 28, 28)\nTrain (60000, 28, 28, 1)\nTest (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "print('TrainX', trainX.shape)\n",
    "print('TestX', trainX.shape)\n",
    "# expand dims to add the channel dimension\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "print('TrainX', trainX.shape)\n",
    "print('TestX', trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TrainX (60000, 10, 2)\nTestX (60000, 10, 2)\n\nConverting to categarical\nTrainX (60000, 10, 2, 2)\nTestX (60000, 10, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print('TrainX', trainY.shape)\n",
    "print('TestX', trainY.shape)\n",
    "# one hot encode target values\n",
    "print(\"\\nConverting to categarical\")\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)\n",
    "print('TrainX', trainY.shape)\n",
    "print('TestX', trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train min=0.000, max=255.000\nTest min=0.000, max=255.000\n"
     ]
    }
   ],
   "source": [
    "# confirm scale of pixels\n",
    "print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches (64): train=938, test=157\nconfirm the scaling works\nBatch shape=(64, 28, 28, 1), min=0.000, max=1.000\n"
     ]
    }
   ],
   "source": [
    "# create generator (1.0/255.0 = 0.003921568627451)\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "# prepare an iterators to scale images\n",
    "batch_size = 64\n",
    "train_iterator = datagen.flow(trainX, trainY, batch_size=batch_size)\n",
    "test_iterator = datagen.flow(testX, testY, batch_size=batch_size)\n",
    "print('Batches (%d): train=%d, test=%d' % (batch_size, len(train_iterator), len(test_iterator)))\n",
    "# confirm the scaling works\n",
    "print(\"confirm the scaling works\")\n",
    "batchX, batchy = train_iterator.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit model with generator"
   ]
  }
 ]
}
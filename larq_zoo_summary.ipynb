{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import larq\n",
    "import larq_zoo\n",
    "from  tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+binary_alexnet stats-------------------------------------------------------------------------------------------+\n| Layer                    Input prec.            Outputs   # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                          x 1       x 1     (kB)                          |\n+---------------------------------------------------------------------------------------------------------------+\n| input_7                            -  (-1, 224, 224, 3)         0         0        0           ?            ? |\n| quant_conv2d_70                    -   (-1, 56, 56, 64)     23232         0     2.84           0     72855552 |\n| max_pooling2d_15                   -   (-1, 27, 27, 64)         0         0        0           0            0 |\n| batch_normalization_93             -   (-1, 27, 27, 64)         0       128     0.50           0            0 |\n| quant_conv2d_71                    1  (-1, 27, 27, 192)    307200         0    37.50   223948800            0 |\n| max_pooling2d_16                   -  (-1, 13, 13, 192)         0         0        0           0            0 |\n| batch_normalization_94             -  (-1, 13, 13, 192)         0       384     1.50           0            0 |\n| quant_conv2d_72                    1  (-1, 13, 13, 384)    663552         0    81.00   112140288            0 |\n| batch_normalization_95             -  (-1, 13, 13, 384)         0       768     3.00           0            0 |\n| quant_conv2d_73                    1  (-1, 13, 13, 384)   1327104         0   162.00   224280576            0 |\n| batch_normalization_96             -  (-1, 13, 13, 384)         0       768     3.00           0            0 |\n| quant_conv2d_74                    1  (-1, 13, 13, 256)    884736         0   108.00   149520384            0 |\n| max_pooling2d_17                   -    (-1, 6, 6, 256)         0         0        0           0            0 |\n| batch_normalization_97             -    (-1, 6, 6, 256)         0       512     2.00           0            0 |\n| flatten_6                          -         (-1, 9216)         0         0        0           0            0 |\n| quant_dense_9                      1         (-1, 4096)  37748736         0  4608.00    37748736            0 |\n| batch_normalization_98             -         (-1, 4096)         0      8192    32.00           0            0 |\n| quant_dense_10                     1         (-1, 4096)  16777216         0  2048.00    16777216            0 |\n| batch_normalization_99             -         (-1, 4096)         0      8192    32.00           0            0 |\n| quant_dense_11                     1         (-1, 1000)   4096000         0   500.00     4096000            0 |\n| batch_normalization_100            -         (-1, 1000)         0      2000     7.81           0            0 |\n| activation_13                      -         (-1, 1000)         0         0        0           ?            ? |\n+---------------------------------------------------------------------------------------------------------------+\n| Total                                                    61827776     20944  7629.15   768512000     72855552 |\n+---------------------------------------------------------------------------------------------------------------+\n+binary_alexnet summary------------------------+\n| Total params                      61.8 M     |\n| Trainable params                  61.8 M     |\n| Non-trainable params              20.9 k     |\n| Model size                        7.45 MiB   |\n| Model size (8-bit FP weights)     7.39 MiB   |\n| Float-32 Equivalent               235.93 MiB |\n| Compression Ratio of Memory       0.03       |\n| Number of MACs                    841 M      |\n| Ratio of MACs that are binarized  0.9134     |\n+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "alexNet = larq_zoo.literature.BinaryAlexNet(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(alexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+birealnet18 stats-----------------------------------------------------------------------------------------------+\n| Layer                    Input prec.             Outputs   # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                           x 1       x 1     (kB)                          |\n+----------------------------------------------------------------------------------------------------------------+\n| input_8                            -   (-1, 224, 224, 3)         0         0        0           ?            ? |\n| conv2d_12                          -  (-1, 112, 112, 64)         0      9408    36.75           0    118013952 |\n| batch_normalization_101            -  (-1, 112, 112, 64)         0       128     0.50           0            0 |\n| max_pooling2d_18                   -    (-1, 56, 56, 64)         0         0        0           0            0 |\n| quant_conv2d_75                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_102            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_32                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| quant_conv2d_76                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_103            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_33                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| quant_conv2d_77                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_104            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_34                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| quant_conv2d_78                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_105            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_35                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| average_pooling2d_9                -    (-1, 28, 28, 64)         0         0        0           0            0 |\n| quant_conv2d_79                    1   (-1, 28, 28, 128)     73728         0     9.00    57802752            0 |\n| conv2d_13                          -   (-1, 28, 28, 128)         0      8192    32.00           0      6422528 |\n| batch_normalization_107            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| batch_normalization_106            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_36                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| quant_conv2d_80                    1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| batch_normalization_108            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_37                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| quant_conv2d_81                    1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| batch_normalization_109            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_38                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| quant_conv2d_82                    1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| batch_normalization_110            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_39                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| average_pooling2d_10               -   (-1, 14, 14, 128)         0         0        0           0            0 |\n| quant_conv2d_83                    1   (-1, 14, 14, 256)    294912         0    36.00    57802752            0 |\n| conv2d_14                          -   (-1, 14, 14, 256)         0     32768   128.00           0      6422528 |\n| batch_normalization_112            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| batch_normalization_111            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_40                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| quant_conv2d_84                    1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| batch_normalization_113            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_41                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| quant_conv2d_85                    1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| batch_normalization_114            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_42                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| quant_conv2d_86                    1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| batch_normalization_115            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_43                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| average_pooling2d_11               -     (-1, 7, 7, 256)         0         0        0           0            0 |\n| quant_conv2d_87                    1     (-1, 7, 7, 512)   1179648         0   144.00    57802752            0 |\n| conv2d_15                          -     (-1, 7, 7, 512)         0    131072   512.00           0      6422528 |\n| batch_normalization_117            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| batch_normalization_116            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_44                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| quant_conv2d_88                    1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| batch_normalization_118            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_45                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| quant_conv2d_89                    1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| batch_normalization_119            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_46                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| quant_conv2d_90                    1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| batch_normalization_120            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_47                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| average_pooling2d_12               -     (-1, 1, 1, 512)         0         0        0           0            0 |\n| flatten_7                          -           (-1, 512)         0         0        0           0            0 |\n| dense_3                            -          (-1, 1000)         0    513000  2003.91           0       512000 |\n| activation_14                      -          (-1, 1000)         0         0        0           ?            ? |\n+----------------------------------------------------------------------------------------------------------------+\n| Total                                                     10985472    704040  4091.16  1676279808    137793536 |\n+----------------------------------------------------------------------------------------------------------------+\n+birealnet18 summary--------------------------+\n| Total params                      11.7 M    |\n| Trainable params                  11.7 M    |\n| Non-trainable params              9.6 k     |\n| Model size                        4.00 MiB  |\n| Model size (8-bit FP weights)     1.98 MiB  |\n| Float-32 Equivalent               44.59 MiB |\n| Compression Ratio of Memory       0.09      |\n| Number of MACs                    1.81 B    |\n| Ratio of MACs that are binarized  0.9240    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "biRealNet = larq_zoo.literature.BiRealNet(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(biRealNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+binary_resnet_e_18 stats----------------------------------------------------------------------------------------+\n| Layer                    Input prec.             Outputs   # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                           x 1       x 1     (kB)                          |\n+----------------------------------------------------------------------------------------------------------------+\n| input_9                            -   (-1, 224, 224, 3)         0         0        0           ?            ? |\n| conv2d_16                          -  (-1, 112, 112, 64)         0      9408    36.75           0    118013952 |\n| batch_normalization_121            -  (-1, 112, 112, 64)         0       128     0.50           0            0 |\n| activation_15                      -  (-1, 112, 112, 64)         0         0        0           ?            ? |\n| max_pooling2d_19                   -    (-1, 56, 56, 64)         0         0        0           0            0 |\n| batch_normalization_122            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| quant_conv2d_91                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_123            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_48                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| quant_conv2d_92                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_124            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_49                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| quant_conv2d_93                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_125            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_50                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| quant_conv2d_94                    1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| batch_normalization_126            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| add_51                             -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| average_pooling2d_13               -    (-1, 28, 28, 64)         0         0        0           0            0 |\n| quant_conv2d_95                    1   (-1, 28, 28, 128)     73728         0     9.00    57802752            0 |\n| conv2d_17                          -   (-1, 28, 28, 128)         0      8192    32.00           0      6422528 |\n| batch_normalization_128            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| batch_normalization_127            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_52                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| quant_conv2d_96                    1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| batch_normalization_129            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_53                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| quant_conv2d_97                    1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| batch_normalization_130            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_54                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| quant_conv2d_98                    1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| batch_normalization_131            -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| add_55                             -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| average_pooling2d_14               -   (-1, 14, 14, 128)         0         0        0           0            0 |\n| quant_conv2d_99                    1   (-1, 14, 14, 256)    294912         0    36.00    57802752            0 |\n| conv2d_18                          -   (-1, 14, 14, 256)         0     32768   128.00           0      6422528 |\n| batch_normalization_133            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| batch_normalization_132            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_56                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| quant_conv2d_100                   1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| batch_normalization_134            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_57                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| quant_conv2d_101                   1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| batch_normalization_135            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_58                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| quant_conv2d_102                   1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| batch_normalization_136            -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| add_59                             -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| average_pooling2d_15               -     (-1, 7, 7, 256)         0         0        0           0            0 |\n| quant_conv2d_103                   1     (-1, 7, 7, 512)   1179648         0   144.00    57802752            0 |\n| conv2d_19                          -     (-1, 7, 7, 512)         0    131072   512.00           0      6422528 |\n| batch_normalization_138            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| batch_normalization_137            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_60                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| quant_conv2d_104                   1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| batch_normalization_139            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_61                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| quant_conv2d_105                   1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| batch_normalization_140            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_62                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| quant_conv2d_106                   1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| batch_normalization_141            -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| add_63                             -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| activation_16                      -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| average_pooling2d_16               -     (-1, 1, 1, 512)         0         0        0           0            0 |\n| flatten_8                          -           (-1, 512)         0         0        0           0            0 |\n| dense_4                            -          (-1, 1000)         0    513000  2003.91           0       512000 |\n| activation_17                      -          (-1, 1000)         0         0        0           ?            ? |\n+----------------------------------------------------------------------------------------------------------------+\n| Total                                                     10985472    704168  4091.66  1676279808    137793536 |\n+----------------------------------------------------------------------------------------------------------------+\n+binary_resnet_e_18 summary-------------------+\n| Total params                      11.7 M    |\n| Trainable params                  11.7 M    |\n| Non-trainable params              9.73 k    |\n| Model size                        4.00 MiB  |\n| Model size (8-bit FP weights)     1.98 MiB  |\n| Float-32 Equivalent               44.59 MiB |\n| Compression Ratio of Memory       0.09      |\n| Number of MACs                    1.81 B    |\n| Ratio of MACs that are binarized  0.9240    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "binaryResNetE18 = larq_zoo.literature.BinaryResNetE18(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(binaryResNetE18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+binary_densenet28 stats----------------------------------------------------------------------------------------+\n| Layer                    Input prec.             Outputs  # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                          x 1       x 1     (kB)                          |\n+---------------------------------------------------------------------------------------------------------------+\n| input_10                           -   (-1, 224, 224, 3)        0         0        0           ?            ? |\n| conv2d_20                          -  (-1, 112, 112, 64)        0      9408    36.75           0    118013952 |\n| batch_normalization_142            -  (-1, 112, 112, 64)        0       128     0.50           0            0 |\n| activation_18                      -  (-1, 112, 112, 64)        0         0        0           ?            ? |\n| max_pooling2d_20                   -    (-1, 56, 56, 64)        0         0        0           0            0 |\n| batch_normalization_143            -    (-1, 56, 56, 64)        0       128     0.50           0            0 |\n| quant_conv2d_107                   1    (-1, 56, 56, 64)    36864         0     4.50   115605504            0 |\n| concatenate_23                     -   (-1, 56, 56, 128)        0         0        0           ?            ? |\n| batch_normalization_144            -   (-1, 56, 56, 128)        0       256     1.00           0            0 |\n| quant_conv2d_108                   1    (-1, 56, 56, 64)    73728         0     9.00   231211008            0 |\n| concatenate_24                     -   (-1, 56, 56, 192)        0         0        0           ?            ? |\n| batch_normalization_145            -   (-1, 56, 56, 192)        0       384     1.50           0            0 |\n| quant_conv2d_109                   1    (-1, 56, 56, 64)   110592         0    13.50   346816512            0 |\n| concatenate_25                     -   (-1, 56, 56, 256)        0         0        0           ?            ? |\n| batch_normalization_146            -   (-1, 56, 56, 256)        0       512     2.00           0            0 |\n| quant_conv2d_110                   1    (-1, 56, 56, 64)   147456         0    18.00   462422016            0 |\n| concatenate_26                     -   (-1, 56, 56, 320)        0         0        0           ?            ? |\n| batch_normalization_147            -   (-1, 56, 56, 320)        0       640     2.50           0            0 |\n| quant_conv2d_111                   1    (-1, 56, 56, 64)   184320         0    22.50   578027520            0 |\n| concatenate_27                     -   (-1, 56, 56, 384)        0         0        0           ?            ? |\n| batch_normalization_148            -   (-1, 56, 56, 384)        0       768     3.00           0            0 |\n| quant_conv2d_112                   1    (-1, 56, 56, 64)   221184         0    27.00   693633024            0 |\n| concatenate_28                     -   (-1, 56, 56, 448)        0         0        0           ?            ? |\n| batch_normalization_149            -   (-1, 56, 56, 448)        0       896     3.50           0            0 |\n| max_pooling2d_21                   -   (-1, 28, 28, 448)        0         0        0           0            0 |\n| activation_19                      -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| conv2d_21                          -   (-1, 28, 28, 160)        0     71680   280.00           0     56197120 |\n| batch_normalization_150            -   (-1, 28, 28, 160)        0       320     1.25           0            0 |\n| quant_conv2d_113                   1    (-1, 28, 28, 64)    92160         0    11.25    72253440            0 |\n| concatenate_29                     -   (-1, 28, 28, 224)        0         0        0           ?            ? |\n| batch_normalization_151            -   (-1, 28, 28, 224)        0       448     1.75           0            0 |\n| quant_conv2d_114                   1    (-1, 28, 28, 64)   129024         0    15.75   101154816            0 |\n| concatenate_30                     -   (-1, 28, 28, 288)        0         0        0           ?            ? |\n| batch_normalization_152            -   (-1, 28, 28, 288)        0       576     2.25           0            0 |\n| quant_conv2d_115                   1    (-1, 28, 28, 64)   165888         0    20.25   130056192            0 |\n| concatenate_31                     -   (-1, 28, 28, 352)        0         0        0           ?            ? |\n| batch_normalization_153            -   (-1, 28, 28, 352)        0       704     2.75           0            0 |\n| quant_conv2d_116                   1    (-1, 28, 28, 64)   202752         0    24.75   158957568            0 |\n| concatenate_32                     -   (-1, 28, 28, 416)        0         0        0           ?            ? |\n| batch_normalization_154            -   (-1, 28, 28, 416)        0       832     3.25           0            0 |\n| quant_conv2d_117                   1    (-1, 28, 28, 64)   239616         0    29.25   187858944            0 |\n| concatenate_33                     -   (-1, 28, 28, 480)        0         0        0           ?            ? |\n| batch_normalization_155            -   (-1, 28, 28, 480)        0       960     3.75           0            0 |\n| quant_conv2d_118                   1    (-1, 28, 28, 64)   276480         0    33.75   216760320            0 |\n| concatenate_34                     -   (-1, 28, 28, 544)        0         0        0           ?            ? |\n| batch_normalization_156            -   (-1, 28, 28, 544)        0      1088     4.25           0            0 |\n| max_pooling2d_22                   -   (-1, 14, 14, 544)        0         0        0           0            0 |\n| activation_20                      -   (-1, 14, 14, 544)        0         0        0           ?            ? |\n| conv2d_22                          -   (-1, 14, 14, 192)        0    104448   408.00           0     20471808 |\n| batch_normalization_157            -   (-1, 14, 14, 192)        0       384     1.50           0            0 |\n| quant_conv2d_119                   1    (-1, 14, 14, 64)   110592         0    13.50    21676032            0 |\n| concatenate_35                     -   (-1, 14, 14, 256)        0         0        0           ?            ? |\n| batch_normalization_158            -   (-1, 14, 14, 256)        0       512     2.00           0            0 |\n| quant_conv2d_120                   1    (-1, 14, 14, 64)   147456         0    18.00    28901376            0 |\n| concatenate_36                     -   (-1, 14, 14, 320)        0         0        0           ?            ? |\n| batch_normalization_159            -   (-1, 14, 14, 320)        0       640     2.50           0            0 |\n| quant_conv2d_121                   1    (-1, 14, 14, 64)   184320         0    22.50    36126720            0 |\n| concatenate_37                     -   (-1, 14, 14, 384)        0         0        0           ?            ? |\n| batch_normalization_160            -   (-1, 14, 14, 384)        0       768     3.00           0            0 |\n| quant_conv2d_122                   1    (-1, 14, 14, 64)   221184         0    27.00    43352064            0 |\n| concatenate_38                     -   (-1, 14, 14, 448)        0         0        0           ?            ? |\n| batch_normalization_161            -   (-1, 14, 14, 448)        0       896     3.50           0            0 |\n| quant_conv2d_123                   1    (-1, 14, 14, 64)   258048         0    31.50    50577408            0 |\n| concatenate_39                     -   (-1, 14, 14, 512)        0         0        0           ?            ? |\n| batch_normalization_162            -   (-1, 14, 14, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_124                   1    (-1, 14, 14, 64)   294912         0    36.00    57802752            0 |\n| concatenate_40                     -   (-1, 14, 14, 576)        0         0        0           ?            ? |\n| batch_normalization_163            -   (-1, 14, 14, 576)        0      1152     4.50           0            0 |\n| max_pooling2d_23                   -     (-1, 7, 7, 576)        0         0        0           0            0 |\n| activation_21                      -     (-1, 7, 7, 576)        0         0        0           ?            ? |\n| conv2d_23                          -     (-1, 7, 7, 256)        0    147456   576.00           0      7225344 |\n| batch_normalization_164            -     (-1, 7, 7, 256)        0       512     2.00           0            0 |\n| quant_conv2d_125                   1      (-1, 7, 7, 64)   147456         0    18.00     7225344            0 |\n| concatenate_41                     -     (-1, 7, 7, 320)        0         0        0           ?            ? |\n| batch_normalization_165            -     (-1, 7, 7, 320)        0       640     2.50           0            0 |\n| quant_conv2d_126                   1      (-1, 7, 7, 64)   184320         0    22.50     9031680            0 |\n| concatenate_42                     -     (-1, 7, 7, 384)        0         0        0           ?            ? |\n| batch_normalization_166            -     (-1, 7, 7, 384)        0       768     3.00           0            0 |\n| quant_conv2d_127                   1      (-1, 7, 7, 64)   221184         0    27.00    10838016            0 |\n| concatenate_43                     -     (-1, 7, 7, 448)        0         0        0           ?            ? |\n| batch_normalization_167            -     (-1, 7, 7, 448)        0       896     3.50           0            0 |\n| quant_conv2d_128                   1      (-1, 7, 7, 64)   258048         0    31.50    12644352            0 |\n| concatenate_44                     -     (-1, 7, 7, 512)        0         0        0           ?            ? |\n| batch_normalization_168            -     (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_129                   1      (-1, 7, 7, 64)   294912         0    36.00    14450688            0 |\n| concatenate_45                     -     (-1, 7, 7, 576)        0         0        0           ?            ? |\n| batch_normalization_169            -     (-1, 7, 7, 576)        0      1152     4.50           0            0 |\n| activation_22                      -     (-1, 7, 7, 576)        0         0        0           ?            ? |\n| average_pooling2d_17               -     (-1, 1, 1, 576)        0         0        0           0            0 |\n| flatten_9                          -           (-1, 576)        0         0        0           0            0 |\n| dense_5                            -          (-1, 1000)        0    577000  2253.91           0       576000 |\n| activation_23                      -          (-1, 1000)        0         0        0           ?            ? |\n+---------------------------------------------------------------------------------------------------------------+\n| Total                                                     4202496    929000  4141.91  3587383296    202484224 |\n+---------------------------------------------------------------------------------------------------------------+\n+binary_densenet28 summary--------------------+\n| Total params                      5.13 M    |\n| Trainable params                  5.11 M    |\n| Non-trainable params              19 k      |\n| Model size                        4.04 MiB  |\n| Model size (8-bit FP weights)     1.39 MiB  |\n| Float-32 Equivalent               19.58 MiB |\n| Compression Ratio of Memory       0.21      |\n| Number of MACs                    3.79 B    |\n| Ratio of MACs that are binarized  0.9466    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "binaryDenseNet28 = larq_zoo.literature.BinaryDenseNet28(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(binaryDenseNet28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+binary_densenet37 stats----------------------------------------------------------------------------------------+\n| Layer                    Input prec.             Outputs  # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                          x 1       x 1     (kB)                          |\n+---------------------------------------------------------------------------------------------------------------+\n| input_11                           -   (-1, 224, 224, 3)        0         0        0           ?            ? |\n| conv2d_24                          -  (-1, 112, 112, 64)        0      9408    36.75           0    118013952 |\n| batch_normalization_170            -  (-1, 112, 112, 64)        0       128     0.50           0            0 |\n| activation_24                      -  (-1, 112, 112, 64)        0         0        0           ?            ? |\n| max_pooling2d_24                   -    (-1, 56, 56, 64)        0         0        0           0            0 |\n| batch_normalization_171            -    (-1, 56, 56, 64)        0       128     0.50           0            0 |\n| quant_conv2d_130                   1    (-1, 56, 56, 64)    36864         0     4.50   115605504            0 |\n| concatenate_46                     -   (-1, 56, 56, 128)        0         0        0           ?            ? |\n| batch_normalization_172            -   (-1, 56, 56, 128)        0       256     1.00           0            0 |\n| quant_conv2d_131                   1    (-1, 56, 56, 64)    73728         0     9.00   231211008            0 |\n| concatenate_47                     -   (-1, 56, 56, 192)        0         0        0           ?            ? |\n| batch_normalization_173            -   (-1, 56, 56, 192)        0       384     1.50           0            0 |\n| quant_conv2d_132                   1    (-1, 56, 56, 64)   110592         0    13.50   346816512            0 |\n| concatenate_48                     -   (-1, 56, 56, 256)        0         0        0           ?            ? |\n| batch_normalization_174            -   (-1, 56, 56, 256)        0       512     2.00           0            0 |\n| quant_conv2d_133                   1    (-1, 56, 56, 64)   147456         0    18.00   462422016            0 |\n| concatenate_49                     -   (-1, 56, 56, 320)        0         0        0           ?            ? |\n| batch_normalization_175            -   (-1, 56, 56, 320)        0       640     2.50           0            0 |\n| quant_conv2d_134                   1    (-1, 56, 56, 64)   184320         0    22.50   578027520            0 |\n| concatenate_50                     -   (-1, 56, 56, 384)        0         0        0           ?            ? |\n| batch_normalization_176            -   (-1, 56, 56, 384)        0       768     3.00           0            0 |\n| quant_conv2d_135                   1    (-1, 56, 56, 64)   221184         0    27.00   693633024            0 |\n| concatenate_51                     -   (-1, 56, 56, 448)        0         0        0           ?            ? |\n| batch_normalization_177            -   (-1, 56, 56, 448)        0       896     3.50           0            0 |\n| max_pooling2d_25                   -   (-1, 28, 28, 448)        0         0        0           0            0 |\n| activation_25                      -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| conv2d_25                          -   (-1, 28, 28, 128)        0     57344   224.00           0     44957696 |\n| batch_normalization_178            -   (-1, 28, 28, 128)        0       256     1.00           0            0 |\n| quant_conv2d_136                   1    (-1, 28, 28, 64)    73728         0     9.00    57802752            0 |\n| concatenate_52                     -   (-1, 28, 28, 192)        0         0        0           ?            ? |\n| batch_normalization_179            -   (-1, 28, 28, 192)        0       384     1.50           0            0 |\n| quant_conv2d_137                   1    (-1, 28, 28, 64)   110592         0    13.50    86704128            0 |\n| concatenate_53                     -   (-1, 28, 28, 256)        0         0        0           ?            ? |\n| batch_normalization_180            -   (-1, 28, 28, 256)        0       512     2.00           0            0 |\n| quant_conv2d_138                   1    (-1, 28, 28, 64)   147456         0    18.00   115605504            0 |\n| concatenate_54                     -   (-1, 28, 28, 320)        0         0        0           ?            ? |\n| batch_normalization_181            -   (-1, 28, 28, 320)        0       640     2.50           0            0 |\n| quant_conv2d_139                   1    (-1, 28, 28, 64)   184320         0    22.50   144506880            0 |\n| concatenate_55                     -   (-1, 28, 28, 384)        0         0        0           ?            ? |\n| batch_normalization_182            -   (-1, 28, 28, 384)        0       768     3.00           0            0 |\n| quant_conv2d_140                   1    (-1, 28, 28, 64)   221184         0    27.00   173408256            0 |\n| concatenate_56                     -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| batch_normalization_183            -   (-1, 28, 28, 448)        0       896     3.50           0            0 |\n| quant_conv2d_141                   1    (-1, 28, 28, 64)   258048         0    31.50   202309632            0 |\n| concatenate_57                     -   (-1, 28, 28, 512)        0         0        0           ?            ? |\n| batch_normalization_184            -   (-1, 28, 28, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_142                   1    (-1, 28, 28, 64)   294912         0    36.00   231211008            0 |\n| concatenate_58                     -   (-1, 28, 28, 576)        0         0        0           ?            ? |\n| batch_normalization_185            -   (-1, 28, 28, 576)        0      1152     4.50           0            0 |\n| quant_conv2d_143                   1    (-1, 28, 28, 64)   331776         0    40.50   260112384            0 |\n| concatenate_59                     -   (-1, 28, 28, 640)        0         0        0           ?            ? |\n| batch_normalization_186            -   (-1, 28, 28, 640)        0      1280     5.00           0            0 |\n| max_pooling2d_26                   -   (-1, 14, 14, 640)        0         0        0           0            0 |\n| activation_26                      -   (-1, 14, 14, 640)        0         0        0           ?            ? |\n| conv2d_26                          -   (-1, 14, 14, 192)        0    122880   480.00           0     24084480 |\n| batch_normalization_187            -   (-1, 14, 14, 192)        0       384     1.50           0            0 |\n| quant_conv2d_144                   1    (-1, 14, 14, 64)   110592         0    13.50    21676032            0 |\n| concatenate_60                     -   (-1, 14, 14, 256)        0         0        0           ?            ? |\n| batch_normalization_188            -   (-1, 14, 14, 256)        0       512     2.00           0            0 |\n| quant_conv2d_145                   1    (-1, 14, 14, 64)   147456         0    18.00    28901376            0 |\n| concatenate_61                     -   (-1, 14, 14, 320)        0         0        0           ?            ? |\n| batch_normalization_189            -   (-1, 14, 14, 320)        0       640     2.50           0            0 |\n| quant_conv2d_146                   1    (-1, 14, 14, 64)   184320         0    22.50    36126720            0 |\n| concatenate_62                     -   (-1, 14, 14, 384)        0         0        0           ?            ? |\n| batch_normalization_190            -   (-1, 14, 14, 384)        0       768     3.00           0            0 |\n| quant_conv2d_147                   1    (-1, 14, 14, 64)   221184         0    27.00    43352064            0 |\n| concatenate_63                     -   (-1, 14, 14, 448)        0         0        0           ?            ? |\n| batch_normalization_191            -   (-1, 14, 14, 448)        0       896     3.50           0            0 |\n| quant_conv2d_148                   1    (-1, 14, 14, 64)   258048         0    31.50    50577408            0 |\n| concatenate_64                     -   (-1, 14, 14, 512)        0         0        0           ?            ? |\n| batch_normalization_192            -   (-1, 14, 14, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_149                   1    (-1, 14, 14, 64)   294912         0    36.00    57802752            0 |\n| concatenate_65                     -   (-1, 14, 14, 576)        0         0        0           ?            ? |\n| batch_normalization_193            -   (-1, 14, 14, 576)        0      1152     4.50           0            0 |\n| quant_conv2d_150                   1    (-1, 14, 14, 64)   331776         0    40.50    65028096            0 |\n| concatenate_66                     -   (-1, 14, 14, 640)        0         0        0           ?            ? |\n| batch_normalization_194            -   (-1, 14, 14, 640)        0      1280     5.00           0            0 |\n| quant_conv2d_151                   1    (-1, 14, 14, 64)   368640         0    45.00    72253440            0 |\n| concatenate_67                     -   (-1, 14, 14, 704)        0         0        0           ?            ? |\n| batch_normalization_195            -   (-1, 14, 14, 704)        0      1408     5.50           0            0 |\n| quant_conv2d_152                   1    (-1, 14, 14, 64)   405504         0    49.50    79478784            0 |\n| concatenate_68                     -   (-1, 14, 14, 768)        0         0        0           ?            ? |\n| batch_normalization_196            -   (-1, 14, 14, 768)        0      1536     6.00           0            0 |\n| quant_conv2d_153                   1    (-1, 14, 14, 64)   442368         0    54.00    86704128            0 |\n| concatenate_69                     -   (-1, 14, 14, 832)        0         0        0           ?            ? |\n| batch_normalization_197            -   (-1, 14, 14, 832)        0      1664     6.50           0            0 |\n| quant_conv2d_154                   1    (-1, 14, 14, 64)   479232         0    58.50    93929472            0 |\n| concatenate_70                     -   (-1, 14, 14, 896)        0         0        0           ?            ? |\n| batch_normalization_198            -   (-1, 14, 14, 896)        0      1792     7.00           0            0 |\n| quant_conv2d_155                   1    (-1, 14, 14, 64)   516096         0    63.00   101154816            0 |\n| concatenate_71                     -   (-1, 14, 14, 960)        0         0        0           ?            ? |\n| batch_normalization_199            -   (-1, 14, 14, 960)        0      1920     7.50           0            0 |\n| max_pooling2d_27                   -     (-1, 7, 7, 960)        0         0        0           0            0 |\n| activation_27                      -     (-1, 7, 7, 960)        0         0        0           ?            ? |\n| conv2d_27                          -     (-1, 7, 7, 256)        0    245760   960.00           0     12042240 |\n| batch_normalization_200            -     (-1, 7, 7, 256)        0       512     2.00           0            0 |\n| quant_conv2d_156                   1      (-1, 7, 7, 64)   147456         0    18.00     7225344            0 |\n| concatenate_72                     -     (-1, 7, 7, 320)        0         0        0           ?            ? |\n| batch_normalization_201            -     (-1, 7, 7, 320)        0       640     2.50           0            0 |\n| quant_conv2d_157                   1      (-1, 7, 7, 64)   184320         0    22.50     9031680            0 |\n| concatenate_73                     -     (-1, 7, 7, 384)        0         0        0           ?            ? |\n| batch_normalization_202            -     (-1, 7, 7, 384)        0       768     3.00           0            0 |\n| quant_conv2d_158                   1      (-1, 7, 7, 64)   221184         0    27.00    10838016            0 |\n| concatenate_74                     -     (-1, 7, 7, 448)        0         0        0           ?            ? |\n| batch_normalization_203            -     (-1, 7, 7, 448)        0       896     3.50           0            0 |\n| quant_conv2d_159                   1      (-1, 7, 7, 64)   258048         0    31.50    12644352            0 |\n| concatenate_75                     -     (-1, 7, 7, 512)        0         0        0           ?            ? |\n| batch_normalization_204            -     (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_160                   1      (-1, 7, 7, 64)   294912         0    36.00    14450688            0 |\n| concatenate_76                     -     (-1, 7, 7, 576)        0         0        0           ?            ? |\n| batch_normalization_205            -     (-1, 7, 7, 576)        0      1152     4.50           0            0 |\n| quant_conv2d_161                   1      (-1, 7, 7, 64)   331776         0    40.50    16257024            0 |\n| concatenate_77                     -     (-1, 7, 7, 640)        0         0        0           ?            ? |\n| batch_normalization_206            -     (-1, 7, 7, 640)        0      1280     5.00           0            0 |\n| activation_28                      -     (-1, 7, 7, 640)        0         0        0           ?            ? |\n| average_pooling2d_18               -     (-1, 1, 1, 640)        0         0        0           0            0 |\n| flatten_10                         -           (-1, 640)        0         0        0           0            0 |\n| dense_6                            -          (-1, 1000)        0    641000  2503.91           0       640000 |\n| activation_29                      -          (-1, 1000)        0         0        0           ?            ? |\n+---------------------------------------------------------------------------------------------------------------+\n| Total                                                     7593984   1108264  5256.16  4506808320    199738368 |\n+---------------------------------------------------------------------------------------------------------------+\n+binary_densenet37 summary--------------------+\n| Total params                      8.7 M     |\n| Trainable params                  8.67 M    |\n| Non-trainable params              31.9 k    |\n| Model size                        5.13 MiB  |\n| Model size (8-bit FP weights)     1.96 MiB  |\n| Float-32 Equivalent               33.20 MiB |\n| Compression Ratio of Memory       0.15      |\n| Number of MACs                    4.71 B    |\n| Ratio of MACs that are binarized  0.9576    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "binaryDenseNet37 = larq_zoo.literature.BinaryDenseNet37(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(binaryDenseNet37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+binary_densenet37_dilated stats--------------------------------------------------------------------------------+\n| Layer                    Input prec.             Outputs  # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                          x 1       x 1     (kB)                          |\n+---------------------------------------------------------------------------------------------------------------+\n| input_12                           -   (-1, 224, 224, 3)        0         0        0           ?            ? |\n| conv2d_28                          -  (-1, 112, 112, 64)        0      9408    36.75           0    118013952 |\n| batch_normalization_207            -  (-1, 112, 112, 64)        0       128     0.50           0            0 |\n| activation_30                      -  (-1, 112, 112, 64)        0         0        0           ?            ? |\n| max_pooling2d_28                   -    (-1, 56, 56, 64)        0         0        0           0            0 |\n| batch_normalization_208            -    (-1, 56, 56, 64)        0       128     0.50           0            0 |\n| quant_conv2d_162                   1    (-1, 56, 56, 64)    36864         0     4.50   115605504            0 |\n| concatenate_78                     -   (-1, 56, 56, 128)        0         0        0           ?            ? |\n| batch_normalization_209            -   (-1, 56, 56, 128)        0       256     1.00           0            0 |\n| quant_conv2d_163                   1    (-1, 56, 56, 64)    73728         0     9.00   231211008            0 |\n| concatenate_79                     -   (-1, 56, 56, 192)        0         0        0           ?            ? |\n| batch_normalization_210            -   (-1, 56, 56, 192)        0       384     1.50           0            0 |\n| quant_conv2d_164                   1    (-1, 56, 56, 64)   110592         0    13.50   346816512            0 |\n| concatenate_80                     -   (-1, 56, 56, 256)        0         0        0           ?            ? |\n| batch_normalization_211            -   (-1, 56, 56, 256)        0       512     2.00           0            0 |\n| quant_conv2d_165                   1    (-1, 56, 56, 64)   147456         0    18.00   462422016            0 |\n| concatenate_81                     -   (-1, 56, 56, 320)        0         0        0           ?            ? |\n| batch_normalization_212            -   (-1, 56, 56, 320)        0       640     2.50           0            0 |\n| quant_conv2d_166                   1    (-1, 56, 56, 64)   184320         0    22.50   578027520            0 |\n| concatenate_82                     -   (-1, 56, 56, 384)        0         0        0           ?            ? |\n| batch_normalization_213            -   (-1, 56, 56, 384)        0       768     3.00           0            0 |\n| quant_conv2d_167                   1    (-1, 56, 56, 64)   221184         0    27.00   693633024            0 |\n| concatenate_83                     -   (-1, 56, 56, 448)        0         0        0           ?            ? |\n| batch_normalization_214            -   (-1, 56, 56, 448)        0       896     3.50           0            0 |\n| max_pooling2d_29                   -   (-1, 28, 28, 448)        0         0        0           0            0 |\n| activation_31                      -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| conv2d_29                          -   (-1, 28, 28, 128)        0     57344   224.00           0     44957696 |\n| batch_normalization_215            -   (-1, 28, 28, 128)        0       256     1.00           0            0 |\n| quant_conv2d_168                   1    (-1, 28, 28, 64)    73728         0     9.00    57802752            0 |\n| concatenate_84                     -   (-1, 28, 28, 192)        0         0        0           ?            ? |\n| batch_normalization_216            -   (-1, 28, 28, 192)        0       384     1.50           0            0 |\n| quant_conv2d_169                   1    (-1, 28, 28, 64)   110592         0    13.50    86704128            0 |\n| concatenate_85                     -   (-1, 28, 28, 256)        0         0        0           ?            ? |\n| batch_normalization_217            -   (-1, 28, 28, 256)        0       512     2.00           0            0 |\n| quant_conv2d_170                   1    (-1, 28, 28, 64)   147456         0    18.00   115605504            0 |\n| concatenate_86                     -   (-1, 28, 28, 320)        0         0        0           ?            ? |\n| batch_normalization_218            -   (-1, 28, 28, 320)        0       640     2.50           0            0 |\n| quant_conv2d_171                   1    (-1, 28, 28, 64)   184320         0    22.50   144506880            0 |\n| concatenate_87                     -   (-1, 28, 28, 384)        0         0        0           ?            ? |\n| batch_normalization_219            -   (-1, 28, 28, 384)        0       768     3.00           0            0 |\n| quant_conv2d_172                   1    (-1, 28, 28, 64)   221184         0    27.00   173408256            0 |\n| concatenate_88                     -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| batch_normalization_220            -   (-1, 28, 28, 448)        0       896     3.50           0            0 |\n| quant_conv2d_173                   1    (-1, 28, 28, 64)   258048         0    31.50   202309632            0 |\n| concatenate_89                     -   (-1, 28, 28, 512)        0         0        0           ?            ? |\n| batch_normalization_221            -   (-1, 28, 28, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_174                   1    (-1, 28, 28, 64)   294912         0    36.00   231211008            0 |\n| concatenate_90                     -   (-1, 28, 28, 576)        0         0        0           ?            ? |\n| batch_normalization_222            -   (-1, 28, 28, 576)        0      1152     4.50           0            0 |\n| quant_conv2d_175                   1    (-1, 28, 28, 64)   331776         0    40.50   260112384            0 |\n| concatenate_91                     -   (-1, 28, 28, 640)        0         0        0           ?            ? |\n| batch_normalization_223            -   (-1, 28, 28, 640)        0      1280     5.00           0            0 |\n| activation_32                      -   (-1, 28, 28, 640)        0         0        0           ?            ? |\n| conv2d_30                          -   (-1, 28, 28, 192)        0    122880   480.00           0     96337920 |\n| batch_normalization_224            -   (-1, 28, 28, 192)        0       384     1.50           0            0 |\n| quant_conv2d_176                   1    (-1, 28, 28, 64)   110592         0    13.50    86704128            0 |\n| concatenate_92                     -   (-1, 28, 28, 256)        0         0        0           ?            ? |\n| batch_normalization_225            -   (-1, 28, 28, 256)        0       512     2.00           0            0 |\n| quant_conv2d_177                   1    (-1, 28, 28, 64)   147456         0    18.00   115605504            0 |\n| concatenate_93                     -   (-1, 28, 28, 320)        0         0        0           ?            ? |\n| batch_normalization_226            -   (-1, 28, 28, 320)        0       640     2.50           0            0 |\n| quant_conv2d_178                   1    (-1, 28, 28, 64)   184320         0    22.50   144506880            0 |\n| concatenate_94                     -   (-1, 28, 28, 384)        0         0        0           ?            ? |\n| batch_normalization_227            -   (-1, 28, 28, 384)        0       768     3.00           0            0 |\n| quant_conv2d_179                   1    (-1, 28, 28, 64)   221184         0    27.00   173408256            0 |\n| concatenate_95                     -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| batch_normalization_228            -   (-1, 28, 28, 448)        0       896     3.50           0            0 |\n| quant_conv2d_180                   1    (-1, 28, 28, 64)   258048         0    31.50   202309632            0 |\n| concatenate_96                     -   (-1, 28, 28, 512)        0         0        0           ?            ? |\n| batch_normalization_229            -   (-1, 28, 28, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_181                   1    (-1, 28, 28, 64)   294912         0    36.00   231211008            0 |\n| concatenate_97                     -   (-1, 28, 28, 576)        0         0        0           ?            ? |\n| batch_normalization_230            -   (-1, 28, 28, 576)        0      1152     4.50           0            0 |\n| quant_conv2d_182                   1    (-1, 28, 28, 64)   331776         0    40.50   260112384            0 |\n| concatenate_98                     -   (-1, 28, 28, 640)        0         0        0           ?            ? |\n| batch_normalization_231            -   (-1, 28, 28, 640)        0      1280     5.00           0            0 |\n| quant_conv2d_183                   1    (-1, 28, 28, 64)   368640         0    45.00   289013760            0 |\n| concatenate_99                     -   (-1, 28, 28, 704)        0         0        0           ?            ? |\n| batch_normalization_232            -   (-1, 28, 28, 704)        0      1408     5.50           0            0 |\n| quant_conv2d_184                   1    (-1, 28, 28, 64)   405504         0    49.50   317915136            0 |\n| concatenate_100                    -   (-1, 28, 28, 768)        0         0        0           ?            ? |\n| batch_normalization_233            -   (-1, 28, 28, 768)        0      1536     6.00           0            0 |\n| quant_conv2d_185                   1    (-1, 28, 28, 64)   442368         0    54.00   346816512            0 |\n| concatenate_101                    -   (-1, 28, 28, 832)        0         0        0           ?            ? |\n| batch_normalization_234            -   (-1, 28, 28, 832)        0      1664     6.50           0            0 |\n| quant_conv2d_186                   1    (-1, 28, 28, 64)   479232         0    58.50   375717888            0 |\n| concatenate_102                    -   (-1, 28, 28, 896)        0         0        0           ?            ? |\n| batch_normalization_235            -   (-1, 28, 28, 896)        0      1792     7.00           0            0 |\n| quant_conv2d_187                   1    (-1, 28, 28, 64)   516096         0    63.00   404619264            0 |\n| concatenate_103                    -   (-1, 28, 28, 960)        0         0        0           ?            ? |\n| batch_normalization_236            -   (-1, 28, 28, 960)        0      1920     7.50           0            0 |\n| activation_33                      -   (-1, 28, 28, 960)        0         0        0           ?            ? |\n| conv2d_31                          -   (-1, 28, 28, 256)        0    245760   960.00           0    192675840 |\n| batch_normalization_237            -   (-1, 28, 28, 256)        0       512     2.00           0            0 |\n| quant_conv2d_188                   1    (-1, 28, 28, 64)   147456         0    18.00   115605504            0 |\n| concatenate_104                    -   (-1, 28, 28, 320)        0         0        0           ?            ? |\n| batch_normalization_238            -   (-1, 28, 28, 320)        0       640     2.50           0            0 |\n| quant_conv2d_189                   1    (-1, 28, 28, 64)   184320         0    22.50   144506880            0 |\n| concatenate_105                    -   (-1, 28, 28, 384)        0         0        0           ?            ? |\n| batch_normalization_239            -   (-1, 28, 28, 384)        0       768     3.00           0            0 |\n| quant_conv2d_190                   1    (-1, 28, 28, 64)   221184         0    27.00   173408256            0 |\n| concatenate_106                    -   (-1, 28, 28, 448)        0         0        0           ?            ? |\n| batch_normalization_240            -   (-1, 28, 28, 448)        0       896     3.50           0            0 |\n| quant_conv2d_191                   1    (-1, 28, 28, 64)   258048         0    31.50   202309632            0 |\n| concatenate_107                    -   (-1, 28, 28, 512)        0         0        0           ?            ? |\n| batch_normalization_241            -   (-1, 28, 28, 512)        0      1024     4.00           0            0 |\n| quant_conv2d_192                   1    (-1, 28, 28, 64)   294912         0    36.00   231211008            0 |\n| concatenate_108                    -   (-1, 28, 28, 576)        0         0        0           ?            ? |\n| batch_normalization_242            -   (-1, 28, 28, 576)        0      1152     4.50           0            0 |\n| quant_conv2d_193                   1    (-1, 28, 28, 64)   331776         0    40.50   260112384            0 |\n| concatenate_109                    -   (-1, 28, 28, 640)        0         0        0           ?            ? |\n| batch_normalization_243            -   (-1, 28, 28, 640)        0      1280     5.00           0            0 |\n| activation_34                      -   (-1, 28, 28, 640)        0         0        0           ?            ? |\n| average_pooling2d_19               -     (-1, 1, 1, 640)        0         0        0           0            0 |\n| flatten_11                         -           (-1, 640)        0         0        0           0            0 |\n| dense_7                            -          (-1, 1000)        0    641000  2503.91           0       640000 |\n| activation_35                      -          (-1, 1000)        0         0        0           ?            ? |\n+---------------------------------------------------------------------------------------------------------------+\n| Total                                                     7593984   1108264  5256.16  7774470144    452625408 |\n+---------------------------------------------------------------------------------------------------------------+\n+binary_densenet37_dilated summary------------+\n| Total params                      8.7 M     |\n| Trainable params                  8.67 M    |\n| Non-trainable params              31.9 k    |\n| Model size                        5.13 MiB  |\n| Model size (8-bit FP weights)     1.96 MiB  |\n| Float-32 Equivalent               33.20 MiB |\n| Compression Ratio of Memory       0.15      |\n| Number of MACs                    8.23 B    |\n| Ratio of MACs that are binarized  0.9450    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "binaryDenseNet37Dilated = larq_zoo.literature.BinaryDenseNet37Dilated(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(binaryDenseNet37Dilated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+binary_densenet45 stats-----------------------------------------------------------------------------------------+\n| Layer                    Input prec.             Outputs   # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                           x 1       x 1     (kB)                          |\n+----------------------------------------------------------------------------------------------------------------+\n| input_13                           -   (-1, 224, 224, 3)         0         0        0           ?            ? |\n| conv2d_32                          -  (-1, 112, 112, 64)         0      9408    36.75           0    118013952 |\n| batch_normalization_244            -  (-1, 112, 112, 64)         0       128     0.50           0            0 |\n| activation_36                      -  (-1, 112, 112, 64)         0         0        0           ?            ? |\n| max_pooling2d_30                   -    (-1, 56, 56, 64)         0         0        0           0            0 |\n| batch_normalization_245            -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| quant_conv2d_194                   1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| concatenate_110                    -   (-1, 56, 56, 128)         0         0        0           ?            ? |\n| batch_normalization_246            -   (-1, 56, 56, 128)         0       256     1.00           0            0 |\n| quant_conv2d_195                   1    (-1, 56, 56, 64)     73728         0     9.00   231211008            0 |\n| concatenate_111                    -   (-1, 56, 56, 192)         0         0        0           ?            ? |\n| batch_normalization_247            -   (-1, 56, 56, 192)         0       384     1.50           0            0 |\n| quant_conv2d_196                   1    (-1, 56, 56, 64)    110592         0    13.50   346816512            0 |\n| concatenate_112                    -   (-1, 56, 56, 256)         0         0        0           ?            ? |\n| batch_normalization_248            -   (-1, 56, 56, 256)         0       512     2.00           0            0 |\n| quant_conv2d_197                   1    (-1, 56, 56, 64)    147456         0    18.00   462422016            0 |\n| concatenate_113                    -   (-1, 56, 56, 320)         0         0        0           ?            ? |\n| batch_normalization_249            -   (-1, 56, 56, 320)         0       640     2.50           0            0 |\n| quant_conv2d_198                   1    (-1, 56, 56, 64)    184320         0    22.50   578027520            0 |\n| concatenate_114                    -   (-1, 56, 56, 384)         0         0        0           ?            ? |\n| batch_normalization_250            -   (-1, 56, 56, 384)         0       768     3.00           0            0 |\n| quant_conv2d_199                   1    (-1, 56, 56, 64)    221184         0    27.00   693633024            0 |\n| concatenate_115                    -   (-1, 56, 56, 448)         0         0        0           ?            ? |\n| batch_normalization_251            -   (-1, 56, 56, 448)         0       896     3.50           0            0 |\n| max_pooling2d_31                   -   (-1, 28, 28, 448)         0         0        0           0            0 |\n| activation_37                      -   (-1, 28, 28, 448)         0         0        0           ?            ? |\n| conv2d_33                          -   (-1, 28, 28, 160)         0     71680   280.00           0     56197120 |\n| batch_normalization_252            -   (-1, 28, 28, 160)         0       320     1.25           0            0 |\n| quant_conv2d_200                   1    (-1, 28, 28, 64)     92160         0    11.25    72253440            0 |\n| concatenate_116                    -   (-1, 28, 28, 224)         0         0        0           ?            ? |\n| batch_normalization_253            -   (-1, 28, 28, 224)         0       448     1.75           0            0 |\n| quant_conv2d_201                   1    (-1, 28, 28, 64)    129024         0    15.75   101154816            0 |\n| concatenate_117                    -   (-1, 28, 28, 288)         0         0        0           ?            ? |\n| batch_normalization_254            -   (-1, 28, 28, 288)         0       576     2.25           0            0 |\n| quant_conv2d_202                   1    (-1, 28, 28, 64)    165888         0    20.25   130056192            0 |\n| concatenate_118                    -   (-1, 28, 28, 352)         0         0        0           ?            ? |\n| batch_normalization_255            -   (-1, 28, 28, 352)         0       704     2.75           0            0 |\n| quant_conv2d_203                   1    (-1, 28, 28, 64)    202752         0    24.75   158957568            0 |\n| concatenate_119                    -   (-1, 28, 28, 416)         0         0        0           ?            ? |\n| batch_normalization_256            -   (-1, 28, 28, 416)         0       832     3.25           0            0 |\n| quant_conv2d_204                   1    (-1, 28, 28, 64)    239616         0    29.25   187858944            0 |\n| concatenate_120                    -   (-1, 28, 28, 480)         0         0        0           ?            ? |\n| batch_normalization_257            -   (-1, 28, 28, 480)         0       960     3.75           0            0 |\n| quant_conv2d_205                   1    (-1, 28, 28, 64)    276480         0    33.75   216760320            0 |\n| concatenate_121                    -   (-1, 28, 28, 544)         0         0        0           ?            ? |\n| batch_normalization_258            -   (-1, 28, 28, 544)         0      1088     4.25           0            0 |\n| quant_conv2d_206                   1    (-1, 28, 28, 64)    313344         0    38.25   245661696            0 |\n| concatenate_122                    -   (-1, 28, 28, 608)         0         0        0           ?            ? |\n| batch_normalization_259            -   (-1, 28, 28, 608)         0      1216     4.75           0            0 |\n| quant_conv2d_207                   1    (-1, 28, 28, 64)    350208         0    42.75   274563072            0 |\n| concatenate_123                    -   (-1, 28, 28, 672)         0         0        0           ?            ? |\n| batch_normalization_260            -   (-1, 28, 28, 672)         0      1344     5.25           0            0 |\n| quant_conv2d_208                   1    (-1, 28, 28, 64)    387072         0    47.25   303464448            0 |\n| concatenate_124                    -   (-1, 28, 28, 736)         0         0        0           ?            ? |\n| batch_normalization_261            -   (-1, 28, 28, 736)         0      1472     5.75           0            0 |\n| quant_conv2d_209                   1    (-1, 28, 28, 64)    423936         0    51.75   332365824            0 |\n| concatenate_125                    -   (-1, 28, 28, 800)         0         0        0           ?            ? |\n| batch_normalization_262            -   (-1, 28, 28, 800)         0      1600     6.25           0            0 |\n| quant_conv2d_210                   1    (-1, 28, 28, 64)    460800         0    56.25   361267200            0 |\n| concatenate_126                    -   (-1, 28, 28, 864)         0         0        0           ?            ? |\n| batch_normalization_263            -   (-1, 28, 28, 864)         0      1728     6.75           0            0 |\n| quant_conv2d_211                   1    (-1, 28, 28, 64)    497664         0    60.75   390168576            0 |\n| concatenate_127                    -   (-1, 28, 28, 928)         0         0        0           ?            ? |\n| batch_normalization_264            -   (-1, 28, 28, 928)         0      1856     7.25           0            0 |\n| max_pooling2d_32                   -   (-1, 14, 14, 928)         0         0        0           0            0 |\n| activation_38                      -   (-1, 14, 14, 928)         0         0        0           ?            ? |\n| conv2d_34                          -   (-1, 14, 14, 288)         0    267264  1044.00           0     52383744 |\n| batch_normalization_265            -   (-1, 14, 14, 288)         0       576     2.25           0            0 |\n| quant_conv2d_212                   1    (-1, 14, 14, 64)    165888         0    20.25    32514048            0 |\n| concatenate_128                    -   (-1, 14, 14, 352)         0         0        0           ?            ? |\n| batch_normalization_266            -   (-1, 14, 14, 352)         0       704     2.75           0            0 |\n| quant_conv2d_213                   1    (-1, 14, 14, 64)    202752         0    24.75    39739392            0 |\n| concatenate_129                    -   (-1, 14, 14, 416)         0         0        0           ?            ? |\n| batch_normalization_267            -   (-1, 14, 14, 416)         0       832     3.25           0            0 |\n| quant_conv2d_214                   1    (-1, 14, 14, 64)    239616         0    29.25    46964736            0 |\n| concatenate_130                    -   (-1, 14, 14, 480)         0         0        0           ?            ? |\n| batch_normalization_268            -   (-1, 14, 14, 480)         0       960     3.75           0            0 |\n| quant_conv2d_215                   1    (-1, 14, 14, 64)    276480         0    33.75    54190080            0 |\n| concatenate_131                    -   (-1, 14, 14, 544)         0         0        0           ?            ? |\n| batch_normalization_269            -   (-1, 14, 14, 544)         0      1088     4.25           0            0 |\n| quant_conv2d_216                   1    (-1, 14, 14, 64)    313344         0    38.25    61415424            0 |\n| concatenate_132                    -   (-1, 14, 14, 608)         0         0        0           ?            ? |\n| batch_normalization_270            -   (-1, 14, 14, 608)         0      1216     4.75           0            0 |\n| quant_conv2d_217                   1    (-1, 14, 14, 64)    350208         0    42.75    68640768            0 |\n| concatenate_133                    -   (-1, 14, 14, 672)         0         0        0           ?            ? |\n| batch_normalization_271            -   (-1, 14, 14, 672)         0      1344     5.25           0            0 |\n| quant_conv2d_218                   1    (-1, 14, 14, 64)    387072         0    47.25    75866112            0 |\n| concatenate_134                    -   (-1, 14, 14, 736)         0         0        0           ?            ? |\n| batch_normalization_272            -   (-1, 14, 14, 736)         0      1472     5.75           0            0 |\n| quant_conv2d_219                   1    (-1, 14, 14, 64)    423936         0    51.75    83091456            0 |\n| concatenate_135                    -   (-1, 14, 14, 800)         0         0        0           ?            ? |\n| batch_normalization_273            -   (-1, 14, 14, 800)         0      1600     6.25           0            0 |\n| quant_conv2d_220                   1    (-1, 14, 14, 64)    460800         0    56.25    90316800            0 |\n| concatenate_136                    -   (-1, 14, 14, 864)         0         0        0           ?            ? |\n| batch_normalization_274            -   (-1, 14, 14, 864)         0      1728     6.75           0            0 |\n| quant_conv2d_221                   1    (-1, 14, 14, 64)    497664         0    60.75    97542144            0 |\n| concatenate_137                    -   (-1, 14, 14, 928)         0         0        0           ?            ? |\n| batch_normalization_275            -   (-1, 14, 14, 928)         0      1856     7.25           0            0 |\n| quant_conv2d_222                   1    (-1, 14, 14, 64)    534528         0    65.25   104767488            0 |\n| concatenate_138                    -   (-1, 14, 14, 992)         0         0        0           ?            ? |\n| batch_normalization_276            -   (-1, 14, 14, 992)         0      1984     7.75           0            0 |\n| quant_conv2d_223                   1    (-1, 14, 14, 64)    571392         0    69.75   111992832            0 |\n| concatenate_139                    -  (-1, 14, 14, 1056)         0         0        0           ?            ? |\n| batch_normalization_277            -  (-1, 14, 14, 1056)         0      2112     8.25           0            0 |\n| quant_conv2d_224                   1    (-1, 14, 14, 64)    608256         0    74.25   119218176            0 |\n| concatenate_140                    -  (-1, 14, 14, 1120)         0         0        0           ?            ? |\n| batch_normalization_278            -  (-1, 14, 14, 1120)         0      2240     8.75           0            0 |\n| quant_conv2d_225                   1    (-1, 14, 14, 64)    645120         0    78.75   126443520            0 |\n| concatenate_141                    -  (-1, 14, 14, 1184)         0         0        0           ?            ? |\n| batch_normalization_279            -  (-1, 14, 14, 1184)         0      2368     9.25           0            0 |\n| max_pooling2d_33                   -    (-1, 7, 7, 1184)         0         0        0           0            0 |\n| activation_39                      -    (-1, 7, 7, 1184)         0         0        0           ?            ? |\n| conv2d_35                          -     (-1, 7, 7, 288)         0    340992  1332.00           0     16708608 |\n| batch_normalization_280            -     (-1, 7, 7, 288)         0       576     2.25           0            0 |\n| quant_conv2d_226                   1      (-1, 7, 7, 64)    165888         0    20.25     8128512            0 |\n| concatenate_142                    -     (-1, 7, 7, 352)         0         0        0           ?            ? |\n| batch_normalization_281            -     (-1, 7, 7, 352)         0       704     2.75           0            0 |\n| quant_conv2d_227                   1      (-1, 7, 7, 64)    202752         0    24.75     9934848            0 |\n| concatenate_143                    -     (-1, 7, 7, 416)         0         0        0           ?            ? |\n| batch_normalization_282            -     (-1, 7, 7, 416)         0       832     3.25           0            0 |\n| quant_conv2d_228                   1      (-1, 7, 7, 64)    239616         0    29.25    11741184            0 |\n| concatenate_144                    -     (-1, 7, 7, 480)         0         0        0           ?            ? |\n| batch_normalization_283            -     (-1, 7, 7, 480)         0       960     3.75           0            0 |\n| quant_conv2d_229                   1      (-1, 7, 7, 64)    276480         0    33.75    13547520            0 |\n| concatenate_145                    -     (-1, 7, 7, 544)         0         0        0           ?            ? |\n| batch_normalization_284            -     (-1, 7, 7, 544)         0      1088     4.25           0            0 |\n| quant_conv2d_230                   1      (-1, 7, 7, 64)    313344         0    38.25    15353856            0 |\n| concatenate_146                    -     (-1, 7, 7, 608)         0         0        0           ?            ? |\n| batch_normalization_285            -     (-1, 7, 7, 608)         0      1216     4.75           0            0 |\n| quant_conv2d_231                   1      (-1, 7, 7, 64)    350208         0    42.75    17160192            0 |\n| concatenate_147                    -     (-1, 7, 7, 672)         0         0        0           ?            ? |\n| batch_normalization_286            -     (-1, 7, 7, 672)         0      1344     5.25           0            0 |\n| quant_conv2d_232                   1      (-1, 7, 7, 64)    387072         0    47.25    18966528            0 |\n| concatenate_148                    -     (-1, 7, 7, 736)         0         0        0           ?            ? |\n| batch_normalization_287            -     (-1, 7, 7, 736)         0      1472     5.75           0            0 |\n| quant_conv2d_233                   1      (-1, 7, 7, 64)    423936         0    51.75    20772864            0 |\n| concatenate_149                    -     (-1, 7, 7, 800)         0         0        0           ?            ? |\n| batch_normalization_288            -     (-1, 7, 7, 800)         0      1600     6.25           0            0 |\n| activation_40                      -     (-1, 7, 7, 800)         0         0        0           ?            ? |\n| average_pooling2d_20               -     (-1, 1, 1, 800)         0         0        0           0            0 |\n| flatten_12                         -           (-1, 800)         0         0        0           0            0 |\n| dense_8                            -          (-1, 1000)         0    801000  3128.91           0       800000 |\n| activation_41                      -          (-1, 1000)         0         0        0           ?            ? |\n+----------------------------------------------------------------------------------------------------------------+\n| Total                                                     12349440   1540072  7523.41  6430556160    244103424 |\n+----------------------------------------------------------------------------------------------------------------+\n+binary_densenet45 summary--------------------+\n| Total params                      13.9 M    |\n| Trainable params                  13.8 M    |\n| Non-trainable params              49.7 k    |\n| Model size                        7.35 MiB  |\n| Model size (8-bit FP weights)     2.94 MiB  |\n| Float-32 Equivalent               52.98 MiB |\n| Compression Ratio of Memory       0.14      |\n| Number of MACs                    6.67 B    |\n| Ratio of MACs that are binarized  0.9634    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "binaryDenseNet45 = larq_zoo.literature.BinaryDenseNet45(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(binaryDenseNet45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using a binary weight quantizer without setting `kernel_constraint` may result in starved weights (where the gradient is always zero).\n",
      "Using a binary weight quantizer without setting `kernel_constraint` may result in starved weights (where the gradient is always zero).\n",
      "Using a binary weight quantizer without setting `kernel_constraint` may result in starved weights (where the gradient is always zero).\n",
      "Using a binary weight quantizer without setting `kernel_constraint` may result in starved weights (where the gradient is always zero).\n",
      "Using a binary weight quantizer without setting `kernel_constraint` may result in starved weights (where the gradient is always zero).\n",
      "Using a binary weight quantizer without setting `kernel_constraint` may result in starved weights (where the gradient is always zero).\n",
      "+dorefanet stats-------------------------------------------------------------------------------------------------+\n",
      "| Layer                    Input prec.            Outputs   # 1-bit  # 32-bit    Memory  2-bit MACs  32-bit MACs |\n",
      "|                                (bit)                          x 1       x 1      (kB)                          |\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "| input_14                           -  (-1, 224, 224, 3)         0         0         0           ?            ? |\n",
      "| conv2d_36                          -   (-1, 54, 54, 96)         0     41568    162.38           0    120932352 |\n",
      "| quant_conv2d_234                   2  (-1, 54, 54, 256)    614400         0     75.00  1791590400            0 |\n",
      "| batch_normalization_289            -  (-1, 54, 54, 256)         0       512      2.00           0            0 |\n",
      "| max_pooling2d_34                   -  (-1, 27, 27, 256)         0         0         0           0            0 |\n",
      "| quant_conv2d_235                   2  (-1, 27, 27, 384)    884736         0    108.00   644972544            0 |\n",
      "| batch_normalization_290            -  (-1, 27, 27, 384)         0       768      3.00           0            0 |\n",
      "| max_pooling2d_35                   -  (-1, 14, 14, 384)         0         0         0           0            0 |\n",
      "| quant_conv2d_236                   2  (-1, 14, 14, 384)   1327104         0    162.00   260112384            0 |\n",
      "| batch_normalization_291            -  (-1, 14, 14, 384)         0       768      3.00           0            0 |\n",
      "| quant_conv2d_237                   2  (-1, 14, 14, 256)    884736         0    108.00   173408256            0 |\n",
      "| batch_normalization_292            -  (-1, 14, 14, 256)         0       512      2.00           0            0 |\n",
      "| max_pooling2d_36                   -    (-1, 6, 6, 256)         0         0         0           0            0 |\n",
      "| flatten_13                         -         (-1, 9216)         0         0         0           0            0 |\n",
      "| quant_dense_12                     2         (-1, 4096)  37748736         0   4608.00    37748736            0 |\n",
      "| batch_normalization_293            -         (-1, 4096)         0      8192     32.00           0            0 |\n",
      "| quant_dense_13                     2         (-1, 4096)  16777216         0   2048.00    16777216            0 |\n",
      "| batch_normalization_294            -         (-1, 4096)         0      8192     32.00           0            0 |\n",
      "| activation_42                      -         (-1, 4096)         0         0         0           ?            ? |\n",
      "| dense_9                            -         (-1, 1000)         0   4097000  16003.91           0      4096000 |\n",
      "| activation_43                      -         (-1, 1000)         0         0         0           ?            ? |\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                    58236928   4157512  23349.28  2924609536    125028352 |\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "+dorefanet summary------------------------------+\n",
      "| Total params                       62.4 M     |\n",
      "| Trainable params                   62.4 M     |\n",
      "| Non-trainable params               18.9 k     |\n",
      "| Model size                         22.80 MiB  |\n",
      "| Model size (8-bit FP weights)      10.91 MiB  |\n",
      "| Float-32 Equivalent                238.02 MiB |\n",
      "| Compression Ratio of Memory        0.10       |\n",
      "| Number of MACs                     3.05 B     |\n",
      "| Ratio of MACs that are ternarized  0.9590     |\n",
      "+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "doReFaNet = larq_zoo.literature.DoReFaNet(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(doReFaNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                  (-1, 28, 28, 64)   202752         0    24.75   158957568            0 |\n| section_1_block_3_dense_concat               -                                                                                                                                         (-1, 28, 28, 416)        0         0        0           ?            ? |\n| section_1_block_3_improve_bn                 -                                                                                                                                         (-1, 28, 28, 416)        0       832     3.25           0            0 |\n| section_1_block_3_improve_binconv            1                                                                                                                                          (-1, 28, 28, 64)   239616         0    29.25   187858944            0 |\n| section_1_block_3_improve_merge              -                                                                                                                                         (-1, 28, 28, 416)        0         0        0           ?            ? |\n| section_1_block_4_dense_bn                   -                                                                                                                                         (-1, 28, 28, 416)        0       832     3.25           0            0 |\n| section_1_block_4_dense_binconv              1                                                                                                                                          (-1, 28, 28, 64)   239616         0    29.25   187858944            0 |\n| section_1_block_4_dense_concat               -                                                                                                                                         (-1, 28, 28, 480)        0         0        0           ?            ? |\n| section_1_block_4_improve_bn                 -                                                                                                                                         (-1, 28, 28, 480)        0       960     3.75           0            0 |\n| section_1_block_4_improve_binconv            1                                                                                                                                          (-1, 28, 28, 64)   276480         0    33.75   216760320            0 |\n| section_1_block_4_improve_merge              -                                                                                                                                         (-1, 28, 28, 480)        0         0        0           ?            ? |\n| section_1_transition_bn                      -                                                                                                                                         (-1, 28, 28, 480)        0       960     3.75           0            0 |\n| section_1_transition_maxpool                 -                                                                                                                                         (-1, 14, 14, 480)        0         0        0           0            0 |\n| section_1_transition_relu                    -                                                                                                                                         (-1, 14, 14, 480)        0         0        0           ?            ? |\n| section_1_transition_pw                      -                                                                                                                                         (-1, 14, 14, 224)        0    107520   420.00           0     21073920 |\n| section_2_block_0_dense_bn                   -                                                                                                                                         (-1, 14, 14, 224)        0       448     1.75           0            0 |\n| section_2_block_0_dense_binconv              1                                                                                                                                          (-1, 14, 14, 64)   129024         0    15.75    25288704            0 |\n| section_2_block_0_dense_concat               -                                                                                                                                         (-1, 14, 14, 288)        0         0        0           ?            ? |\n| section_2_block_0_improve_bn                 -                                                                                                                                         (-1, 14, 14, 288)        0       576     2.25           0            0 |\n| section_2_block_0_improve_binconv            1                                                                                                                                          (-1, 14, 14, 64)   165888         0    20.25    32514048            0 |\n| section_2_block_0_improve_merge              -                                                                                                                                         (-1, 14, 14, 288)        0         0        0           ?            ? |\n| section_2_block_1_dense_bn                   -                                                                                                                                         (-1, 14, 14, 288)        0       576     2.25           0            0 |\n| section_2_block_1_dense_binconv              1                                                                                                                                          (-1, 14, 14, 64)   165888         0    20.25    32514048            0 |\n| section_2_block_1_dense_concat               -                                                                                                                                         (-1, 14, 14, 352)        0         0        0           ?            ? |\n| section_2_block_1_improve_bn                 -                                                                                                                                         (-1, 14, 14, 352)        0       704     2.75           0            0 |\n| section_2_block_1_improve_binconv            1                                                                                                                                          (-1, 14, 14, 64)   202752         0    24.75    39739392            0 |\n| section_2_block_1_improve_merge              -                                                                                                                                         (-1, 14, 14, 352)        0         0        0           ?            ? |\n| section_2_block_2_dense_bn                   -                                                                                                                                         (-1, 14, 14, 352)        0       704     2.75           0            0 |\n| section_2_block_2_dense_binconv              1                                                                                                                                          (-1, 14, 14, 64)   202752         0    24.75    39739392            0 |\n| section_2_block_2_dense_concat               -                                                                                                                                         (-1, 14, 14, 416)        0         0        0           ?            ? |\n| section_2_block_2_improve_bn                 -                                                                                                                                         (-1, 14, 14, 416)        0       832     3.25           0            0 |\n| section_2_block_2_improve_binconv            1                                                                                                                                          (-1, 14, 14, 64)   239616         0    29.25    46964736            0 |\n| section_2_block_2_improve_merge              -                                                                                                                                         (-1, 14, 14, 416)        0         0        0           ?            ? |\n| section_2_block_3_dense_bn                   -                                                                                                                                         (-1, 14, 14, 416)        0       832     3.25           0            0 |\n| section_2_block_3_dense_binconv              1                                                                                                                                          (-1, 14, 14, 64)   239616         0    29.25    46964736            0 |\n| section_2_block_3_dense_concat               -                                                                                                                                         (-1, 14, 14, 480)        0         0        0           ?            ? |\n| section_2_block_3_improve_bn                 -                                                                                                                                         (-1, 14, 14, 480)        0       960     3.75           0            0 |\n| section_2_block_3_improve_binconv            1                                                                                                                                          (-1, 14, 14, 64)   276480         0    33.75    54190080            0 |\n| section_2_block_3_improve_merge              -                                                                                                                                         (-1, 14, 14, 480)        0         0        0           ?            ? |\n| section_2_transition_bn                      -                                                                                                                                         (-1, 14, 14, 480)        0       960     3.75           0            0 |\n| section_2_transition_maxpool                 -                                                                                                                                           (-1, 7, 7, 480)        0         0        0           0            0 |\n| section_2_transition_relu                    -                                                                                                                                           (-1, 7, 7, 480)        0         0        0           ?            ? |\n| section_2_transition_pw                      -                                                                                                                                           (-1, 7, 7, 256)        0    122880   480.00           0      6021120 |\n| section_3_block_0_dense_bn                   -                                                                                                                                           (-1, 7, 7, 256)        0       512     2.00           0            0 |\n| section_3_block_0_dense_binconv              1                                                                                                                                            (-1, 7, 7, 64)   147456         0    18.00     7225344            0 |\n| section_3_block_0_dense_concat               -                                                                                                                                           (-1, 7, 7, 320)        0         0        0           ?            ? |\n| section_3_block_0_improve_bn                 -                                                                                                                                           (-1, 7, 7, 320)        0       640     2.50           0            0 |\n| section_3_block_0_improve_binconv            1                                                                                                                                            (-1, 7, 7, 64)   184320         0    22.50     9031680            0 |\n| section_3_block_0_improve_merge              -                                                                                                                                           (-1, 7, 7, 320)        0         0        0           ?            ? |\n| section_3_block_1_dense_bn                   -                                                                                                                                           (-1, 7, 7, 320)        0       640     2.50           0            0 |\n| section_3_block_1_dense_binconv              1                                                                                                                                            (-1, 7, 7, 64)   184320         0    22.50     9031680            0 |\n| section_3_block_1_dense_concat               -                                                                                                                                           (-1, 7, 7, 384)        0         0        0           ?            ? |\n| section_3_block_1_improve_bn                 -                                                                                                                                           (-1, 7, 7, 384)        0       768     3.00           0            0 |\n| section_3_block_1_improve_binconv            1                                                                                                                                            (-1, 7, 7, 64)   221184         0    27.00    10838016            0 |\n| section_3_block_1_improve_merge              -                                                                                                                                           (-1, 7, 7, 384)        0         0        0           ?            ? |\n| section_3_block_2_dense_bn                   -                                                                                                                                           (-1, 7, 7, 384)        0       768     3.00           0            0 |\n| section_3_block_2_dense_binconv              1                                                                                                                                            (-1, 7, 7, 64)   221184         0    27.00    10838016            0 |\n| section_3_block_2_dense_concat               -                                                                                                                                           (-1, 7, 7, 448)        0         0        0           ?            ? |\n| section_3_block_2_improve_bn                 -                                                                                                                                           (-1, 7, 7, 448)        0       896     3.50           0            0 |\n| section_3_block_2_improve_binconv            1                                                                                                                                            (-1, 7, 7, 64)   258048         0    31.50    12644352            0 |\n| section_3_block_2_improve_merge              -                                                                                                                                           (-1, 7, 7, 448)        0         0        0           ?            ? |\n| section_3_block_3_dense_bn                   -                                                                                                                                           (-1, 7, 7, 448)        0       896     3.50           0            0 |\n| section_3_block_3_dense_binconv              1                                                                                                                                            (-1, 7, 7, 64)   258048         0    31.50    12644352            0 |\n| section_3_block_3_dense_concat               -                                                                                                                                           (-1, 7, 7, 512)        0         0        0           ?            ? |\n| section_3_block_3_improve_bn                 -                                                                                                                                           (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n| section_3_block_3_improve_binconv            1                                                                                                                                            (-1, 7, 7, 64)   294912         0    36.00    14450688            0 |\n| section_3_block_3_improve_merge              -                                                                                                                                           (-1, 7, 7, 512)        0         0        0           ?            ? |\n| head_bn                                      -                                                                                                                                           (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n| head_relu                                    -                                                                                                                                           (-1, 7, 7, 512)        0         0        0           ?            ? |\n| head_globalpool_pool                         -                                                                                                                                           (-1, 1, 1, 512)        0         0        0           0            0 |\n| head_globalpool_flatten                      -                                                                                                                                                 (-1, 512)        0         0        0           0            0 |\n| head_dense                                   -                                                                                                                                                (-1, 1000)        0    513000  2003.91           0       512000 |\n| head_softmax                                 -                                                                                                                                                (-1, 1000)        0         0        0           ?            ? |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Total                                                                                                                                                                                                     6119424    825160  3970.28  4624220160    136388608 |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n+meliusnet22 summary--------------------------+\n| Total params                      6.94 M    |\n| Trainable params                  6.92 M    |\n| Non-trainable params              25.1 k    |\n| Model size                        3.88 MiB  |\n| Model size (8-bit FP weights)     1.52 MiB  |\n| Float-32 Equivalent               26.49 MiB |\n| Compression Ratio of Memory       0.15      |\n| Number of MACs                    4.76 B    |\n| Ratio of MACs that are binarized  0.9714    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "meliusNet22 = larq_zoo.literature.MeliusNet22(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(meliusNet22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      -            (-1, 64)         0       512     2.00           0          512 |\n| r2b_bnn_block_3b_conv2d                            1    (-1, 56, 56, 64)     36864         0     4.50   115605504            0 |\n| r2b_bnn_block_3b_scaling_reshape                   -      (-1, 1, 1, 64)         0         0        0           ?            ? |\n| r2b_bnn_block_3b_scaling_multiplication            -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| r2b_bnn_block_3b_prelu                             -    (-1, 56, 56, 64)         0        64     0.25           ?            ? |\n| r2b_bnn_block_3b_skip_add                          -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| r2b_bnn_block_3_out                                -    (-1, 56, 56, 64)         0         0        0           ?            ? |\n| r2b_bnn_block_4a_batch_norm                        -    (-1, 56, 56, 64)         0       128     0.50           0            0 |\n| r2b_bnn_block_4a_scaling_pool_pool                 -      (-1, 1, 1, 64)         0         0        0           0            0 |\n| r2b_bnn_block_4a_scaling_pool_flatten              -            (-1, 64)         0         0        0           0            0 |\n| r2b_bnn_block_4a_scaling_dense_reduce              -             (-1, 8)         0       512     2.00           0          512 |\n| r2b_bnn_block_4a_scaling_dense_expand              -           (-1, 128)         0      1024     4.00           0         1024 |\n| r2b_bnn_block_4a_conv2d                            1   (-1, 28, 28, 128)     73728         0     9.00    57802752            0 |\n| r2b_bnn_block_4a_scaling_reshape                   -     (-1, 1, 1, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_4a_shortcut_pool                     -    (-1, 28, 28, 64)         0         0        0           0            0 |\n| r2b_bnn_block_4a_scaling_multiplication            -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_4a_shortcut_conv2d                   -   (-1, 28, 28, 128)         0      8192    32.00           0      6422528 |\n| r2b_bnn_block_4a_prelu                             -   (-1, 28, 28, 128)         0       128     0.50           ?            ? |\n| r2b_bnn_block_4a_shortcut_batch_norm               -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| r2b_bnn_block_4a_skip_add                          -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_4b_batch_norm                        -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| r2b_bnn_block_4b_scaling_pool_pool                 -     (-1, 1, 1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_4b_scaling_pool_flatten              -           (-1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_4b_scaling_dense_reduce              -            (-1, 16)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_4b_scaling_dense_expand              -           (-1, 128)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_4b_conv2d                            1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| r2b_bnn_block_4b_scaling_reshape                   -     (-1, 1, 1, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_4b_scaling_multiplication            -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_4b_prelu                             -   (-1, 28, 28, 128)         0       128     0.50           ?            ? |\n| r2b_bnn_block_4b_skip_add                          -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_4_out                                -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5a_batch_norm                        -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| r2b_bnn_block_5a_scaling_pool_pool                 -     (-1, 1, 1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_5a_scaling_pool_flatten              -           (-1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_5a_scaling_dense_reduce              -            (-1, 16)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_5a_scaling_dense_expand              -           (-1, 128)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_5a_conv2d                            1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| r2b_bnn_block_5a_scaling_reshape                   -     (-1, 1, 1, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5a_scaling_multiplication            -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5a_prelu                             -   (-1, 28, 28, 128)         0       128     0.50           ?            ? |\n| r2b_bnn_block_5a_skip_add                          -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5b_batch_norm                        -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| r2b_bnn_block_5b_scaling_pool_pool                 -     (-1, 1, 1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_5b_scaling_pool_flatten              -           (-1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_5b_scaling_dense_reduce              -            (-1, 16)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_5b_scaling_dense_expand              -           (-1, 128)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_5b_conv2d                            1   (-1, 28, 28, 128)    147456         0    18.00   115605504            0 |\n| r2b_bnn_block_5b_scaling_reshape                   -     (-1, 1, 1, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5b_scaling_multiplication            -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5b_prelu                             -   (-1, 28, 28, 128)         0       128     0.50           ?            ? |\n| r2b_bnn_block_5b_skip_add                          -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_5_out                                -   (-1, 28, 28, 128)         0         0        0           ?            ? |\n| r2b_bnn_block_6a_batch_norm                        -   (-1, 28, 28, 128)         0       256     1.00           0            0 |\n| r2b_bnn_block_6a_scaling_pool_pool                 -     (-1, 1, 1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_6a_scaling_pool_flatten              -           (-1, 128)         0         0        0           0            0 |\n| r2b_bnn_block_6a_scaling_dense_reduce              -            (-1, 16)         0      2048     8.00           0         2048 |\n| r2b_bnn_block_6a_scaling_dense_expand              -           (-1, 256)         0      4096    16.00           0         4096 |\n| r2b_bnn_block_6a_conv2d                            1   (-1, 14, 14, 256)    294912         0    36.00    57802752            0 |\n| r2b_bnn_block_6a_scaling_reshape                   -     (-1, 1, 1, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_6a_shortcut_pool                     -   (-1, 14, 14, 128)         0         0        0           0            0 |\n| r2b_bnn_block_6a_scaling_multiplication            -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_6a_shortcut_conv2d                   -   (-1, 14, 14, 256)         0     32768   128.00           0      6422528 |\n| r2b_bnn_block_6a_prelu                             -   (-1, 14, 14, 256)         0       256     1.00           ?            ? |\n| r2b_bnn_block_6a_shortcut_batch_norm               -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| r2b_bnn_block_6a_skip_add                          -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_6b_batch_norm                        -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| r2b_bnn_block_6b_scaling_pool_pool                 -     (-1, 1, 1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_6b_scaling_pool_flatten              -           (-1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_6b_scaling_dense_reduce              -            (-1, 32)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_6b_scaling_dense_expand              -           (-1, 256)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_6b_conv2d                            1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| r2b_bnn_block_6b_scaling_reshape                   -     (-1, 1, 1, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_6b_scaling_multiplication            -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_6b_prelu                             -   (-1, 14, 14, 256)         0       256     1.00           ?            ? |\n| r2b_bnn_block_6b_skip_add                          -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_6_out                                -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7a_batch_norm                        -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| r2b_bnn_block_7a_scaling_pool_pool                 -     (-1, 1, 1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_7a_scaling_pool_flatten              -           (-1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_7a_scaling_dense_reduce              -            (-1, 32)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_7a_scaling_dense_expand              -           (-1, 256)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_7a_conv2d                            1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| r2b_bnn_block_7a_scaling_reshape                   -     (-1, 1, 1, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7a_scaling_multiplication            -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7a_prelu                             -   (-1, 14, 14, 256)         0       256     1.00           ?            ? |\n| r2b_bnn_block_7a_skip_add                          -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7b_batch_norm                        -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| r2b_bnn_block_7b_scaling_pool_pool                 -     (-1, 1, 1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_7b_scaling_pool_flatten              -           (-1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_7b_scaling_dense_reduce              -            (-1, 32)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_7b_scaling_dense_expand              -           (-1, 256)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_7b_conv2d                            1   (-1, 14, 14, 256)    589824         0    72.00   115605504            0 |\n| r2b_bnn_block_7b_scaling_reshape                   -     (-1, 1, 1, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7b_scaling_multiplication            -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7b_prelu                             -   (-1, 14, 14, 256)         0       256     1.00           ?            ? |\n| r2b_bnn_block_7b_skip_add                          -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_7_out                                -   (-1, 14, 14, 256)         0         0        0           ?            ? |\n| r2b_bnn_block_8a_batch_norm                        -   (-1, 14, 14, 256)         0       512     2.00           0            0 |\n| r2b_bnn_block_8a_scaling_pool_pool                 -     (-1, 1, 1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_8a_scaling_pool_flatten              -           (-1, 256)         0         0        0           0            0 |\n| r2b_bnn_block_8a_scaling_dense_reduce              -            (-1, 32)         0      8192    32.00           0         8192 |\n| r2b_bnn_block_8a_scaling_dense_expand              -           (-1, 512)         0     16384    64.00           0        16384 |\n| r2b_bnn_block_8a_conv2d                            1     (-1, 7, 7, 512)   1179648         0   144.00    57802752            0 |\n| r2b_bnn_block_8a_scaling_reshape                   -     (-1, 1, 1, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_8a_shortcut_pool                     -     (-1, 7, 7, 256)         0         0        0           0            0 |\n| r2b_bnn_block_8a_scaling_multiplication            -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_8a_shortcut_conv2d                   -     (-1, 7, 7, 512)         0    131072   512.00           0      6422528 |\n| r2b_bnn_block_8a_prelu                             -     (-1, 7, 7, 512)         0       512     2.00           ?            ? |\n| r2b_bnn_block_8a_shortcut_batch_norm               -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| r2b_bnn_block_8a_skip_add                          -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_8b_batch_norm                        -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| r2b_bnn_block_8b_scaling_pool_pool                 -     (-1, 1, 1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_8b_scaling_pool_flatten              -           (-1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_8b_scaling_dense_reduce              -            (-1, 64)         0     32768   128.00           0        32768 |\n| r2b_bnn_block_8b_scaling_dense_expand              -           (-1, 512)         0     32768   128.00           0        32768 |\n| r2b_bnn_block_8b_conv2d                            1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| r2b_bnn_block_8b_scaling_reshape                   -     (-1, 1, 1, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_8b_scaling_multiplication            -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_8b_prelu                             -     (-1, 7, 7, 512)         0       512     2.00           ?            ? |\n| r2b_bnn_block_8b_skip_add                          -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_8_out                                -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9a_batch_norm                        -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| r2b_bnn_block_9a_scaling_pool_pool                 -     (-1, 1, 1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_9a_scaling_pool_flatten              -           (-1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_9a_scaling_dense_reduce              -            (-1, 64)         0     32768   128.00           0        32768 |\n| r2b_bnn_block_9a_scaling_dense_expand              -           (-1, 512)         0     32768   128.00           0        32768 |\n| r2b_bnn_block_9a_conv2d                            1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| r2b_bnn_block_9a_scaling_reshape                   -     (-1, 1, 1, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9a_scaling_multiplication            -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9a_prelu                             -     (-1, 7, 7, 512)         0       512     2.00           ?            ? |\n| r2b_bnn_block_9a_skip_add                          -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9b_batch_norm                        -     (-1, 7, 7, 512)         0      1024     4.00           0            0 |\n| r2b_bnn_block_9b_scaling_pool_pool                 -     (-1, 1, 1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_9b_scaling_pool_flatten              -           (-1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_9b_scaling_dense_reduce              -            (-1, 64)         0     32768   128.00           0        32768 |\n| r2b_bnn_block_9b_scaling_dense_expand              -           (-1, 512)         0     32768   128.00           0        32768 |\n| r2b_bnn_block_9b_conv2d                            1     (-1, 7, 7, 512)   2359296         0   288.00   115605504            0 |\n| r2b_bnn_block_9b_scaling_reshape                   -     (-1, 1, 1, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9b_scaling_multiplication            -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9b_prelu                             -     (-1, 7, 7, 512)         0       512     2.00           ?            ? |\n| r2b_bnn_block_9b_skip_add                          -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_9_out                                -     (-1, 7, 7, 512)         0         0        0           ?            ? |\n| r2b_bnn_block_10_global_pool_pool                  -     (-1, 1, 1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_10_global_pool_flatten               -           (-1, 512)         0         0        0           0            0 |\n| r2b_bnn_block_10_logits                            -          (-1, 1000)         0    513000  2003.91           0       512000 |\n| r2b_bnn_block_10_probs                             -          (-1, 1000)         0         0        0           ?            ? |\n+--------------------------------------------------------------------------------------------------------------------------------+\n| Total                                                                     10985472   1001448  5252.91  1676279808    138087936 |\n+--------------------------------------------------------------------------------------------------------------------------------+\n+r2b_bnn summary------------------------------+\n| Total params                      12 M      |\n| Trainable params                  12 M      |\n| Non-trainable params              8.7 k     |\n| Model size                        5.13 MiB  |\n| Model size (8-bit FP weights)     2.26 MiB  |\n| Float-32 Equivalent               45.73 MiB |\n| Compression Ratio of Memory       0.11      |\n| Number of MACs                    1.81 B    |\n| Ratio of MACs that are binarized  0.9239    |\n+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "realToBinaryNet = larq_zoo.literature.RealToBinaryNet(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(realToBinaryNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+xnornet stats---------------------------------------------------------------------------------------------------+\n| Layer                    Input prec.            Outputs   # 1-bit  # 32-bit    Memory  1-bit MACs  32-bit MACs |\n|                                (bit)                          x 1       x 1      (kB)                          |\n+----------------------------------------------------------------------------------------------------------------+\n| input_17                           -  (-1, 224, 224, 3)         0         0         0           ?            ? |\n| conv2d_37                          -   (-1, 56, 56, 96)         0     34848    136.12           0    109283328 |\n| batch_normalization_295            -   (-1, 56, 56, 96)         0       192      0.75           0            0 |\n| activation_44                      -   (-1, 56, 56, 96)         0         0         0           ?            ? |\n| max_pooling2d_37                   -   (-1, 27, 27, 96)         0         0         0           0            0 |\n| batch_normalization_296            -   (-1, 27, 27, 96)         0       192      0.75           0            0 |\n| quant_conv2d_238                   1  (-1, 27, 27, 256)    614400         0     75.00   447897600            0 |\n| max_pooling2d_38                   -  (-1, 13, 13, 256)         0         0         0           0            0 |\n| batch_normalization_297            -  (-1, 13, 13, 256)         0       512      2.00           0            0 |\n| quant_conv2d_239                   1  (-1, 13, 13, 384)    884736         0    108.00   149520384            0 |\n| batch_normalization_298            -  (-1, 13, 13, 384)         0       768      3.00           0            0 |\n| quant_conv2d_240                   1  (-1, 13, 13, 384)   1327104         0    162.00   224280576            0 |\n| batch_normalization_299            -  (-1, 13, 13, 384)         0       768      3.00           0            0 |\n| quant_conv2d_241                   1  (-1, 13, 13, 256)    884736         0    108.00   149520384            0 |\n| max_pooling2d_39                   -    (-1, 6, 6, 256)         0         0         0           0            0 |\n| batch_normalization_300            -    (-1, 6, 6, 256)         0       512      2.00           0            0 |\n| quant_conv2d_242                   1   (-1, 1, 1, 4096)  37748736         0   4608.00    37748736            0 |\n| batch_normalization_301            -   (-1, 1, 1, 4096)         0      8192     32.00           0            0 |\n| quant_conv2d_243                   1   (-1, 1, 1, 4096)  16777216         0   2048.00    16777216            0 |\n| batch_normalization_302            -   (-1, 1, 1, 4096)         0      8192     32.00           0            0 |\n| activation_45                      -   (-1, 1, 1, 4096)         0         0         0           ?            ? |\n| flatten_14                         -         (-1, 4096)         0         0         0           0            0 |\n| dense_10                           -         (-1, 1000)         0   4096000  16000.00           0      4096000 |\n| activation_46                      -         (-1, 1000)         0         0         0           ?            ? |\n+----------------------------------------------------------------------------------------------------------------+\n| Total                                                    58236928   4150176  23320.62  1025744896    113379328 |\n+----------------------------------------------------------------------------------------------------------------+\n+xnornet summary-------------------------------+\n| Total params                      62.4 M     |\n| Trainable params                  62.4 M     |\n| Non-trainable params              19.3 k     |\n| Model size                        22.77 MiB  |\n| Model size (8-bit FP weights)     10.90 MiB  |\n| Float-32 Equivalent               237.99 MiB |\n| Compression Ratio of Memory       0.10       |\n| Number of MACs                    1.14 B     |\n| Ratio of MACs that are binarized  0.9005     |\n+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "xNORNet = larq_zoo.literature.XNORNet(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    num_classes=1000\n",
    ")\n",
    "larq.models.summary(xNORNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-96e6a510ee3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m vgg16 = VGG16(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# Classification block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'flatten'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fc1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fc2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2641\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2643\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2644\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2645\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1169\u001b[0m                        'should be defined. Found `None`.')\n\u001b[0;32m   1170\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlast_dim\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m     self.kernel = self.add_weight(\n\u001b[0m\u001b[0;32m   1172\u001b[0m         \u001b[1;34m'kernel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlast_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mcaching_device\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m     variable = self._add_variable_with_custom_getter(\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint_initializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m     new_variable = getter(\n\u001b[0m\u001b[0;32m    746\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[1;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[1;31m# can remove the V1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   \u001b[0mvariable_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m   return tf_variables.VariableV1(\n\u001b[0m\u001b[0;32m    134\u001b[0m       \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[1;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     return previous_getter(\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                         shape=None):\n\u001b[0;32m    198\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2581\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m     \u001b[0mdistribute_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"distribute_strategy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2583\u001b[1;33m     return resource_variable_ops.ResourceVariable(\n\u001b[0m\u001b[0;32m   2584\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2585\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1505\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m       self._init_from_args(\n\u001b[0m\u001b[0;32m   1508\u001b[0m           \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[0;32m   1649\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initializer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[1;32m-> 1651\u001b[1;33m                 \u001b[0minitial_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[0;32m   1653\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    395\u001b[0m        \u001b[1;33m(\u001b[0m\u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_get_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[0;32m   1041\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m       \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m     return op(\n\u001b[0m\u001b[0;32m   1044\u001b[0m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[0;32m    299\u001b[0m           shape, minval, maxval, seed=seed1, seed2=seed2, name=name)\n\u001b[0;32m    300\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m       result = gen_random_ops.random_uniform(\n\u001b[0m\u001b[0;32m    302\u001b[0m           shape, dtype, seed=seed1, seed2=seed2)\n\u001b[0;32m    303\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mminval_is_zero\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[0;32m    724\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "vgg16 = VGG16(\n",
    "    input_shape=None,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    include_top=True\n",
    ")\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}